## Functional Requirements

BabyCoach is an AI assistant that helps a user interact with their personal Evernote corpus. It is designed to:

- Ingest and index thousands of notes exported from Evernote (via `.enex` or `.json` format), using a Retrieval-Augmented Generation (RAG) approach.
- Accept natural language queries from the note owner through a simple interface.
- Return helpful, accurate, and context-aware responses by synthesizing information across the entire corpus.

The goal is to make querying one’s personal archive feel more like a conversation with a well-informed assistant than a manual search task.

## Assumptions & Technical Uncertainty

At this stage, we assume that at least one of the following implementation paths is viable for running BabyCoach:

- **Local model execution on a Mac**  
  It may be possible to run a small model and vector store locally for development.  
  - *Risks*: Limited memory or CPU/GPU resources may lead to slow performance or outright failure.
  - *Risks*: Development work may not translate cleanly to a future deployment on larger or hosted models.

- **Integration with commercial hosted models (e.g. ChatGPT, Claude)**  
  The system may work by streaming queries and note chunks to a hosted LLM service under an existing subscription.  
  - *Risks*: Session context limits, re-uploading the corpus, or poor long-term context handling may reduce usability.
  - *Risks*: Less compelling as a technical portfolio project if relying too heavily on hosted services.

- **Development using free-tier cloud compute (e.g. Colab, Lightning AI, SageMaker Studio Lab)**  
  Development and testing could be done using no-cost compute platforms.  
  - *Risks*: Environment setup and login routines could slow iteration.
  - *Risks*: Interfaces may be too clunky or restrictive for productive work.

- **Deployment on rented cloud infrastructure (e.g. AWS, GCP, Azure)**  
  A cloud VM or containerized app could provide consistent, scalable runtime behavior.  
  - *Risks*: Nontrivial setup and infrastructure overhead.
  - *Risks*: Potential for unnecessary costs or accidental overspending.

Overall, the system assumes that the corpus is small enough (thousands of notes) to make RAG feasible, but large enough to require nontrivial retrieval strategies and efficient infrastructure decisions.

## Data Strategy

BabyCoach operates on three main data types:

1. **Queries**: Natural language strings submitted by the user.
2. **Corpus**: Thousands of exported Evernote notes stored as structured JSON. This serves as the underlying knowledge base for retrieval and context injection.
3. **Responses**: Natural language output generated by the model, typically 1–3 paragraphs, though potentially longer depending on future use cases.

The note corpus has already been exported and cleaned into a structured JSON format (~8MB, ~25,000 lines), with fields for text, title, tags, and metadata. This corpus is assumed to be static during MVP development.

We anticipate loading these notes into a vector store via a RAG pipeline. At this stage, the exact exposure model (chunk size, overlap, metadata filtering, semantic indexing strategy) is undecided and expected to evolve through iteration. This is a relatively mature area in the GenAI ecosystem, and we assume a working RAG setup will be achievable with existing libraries (e.g., LangChain, LlamaIndex, Haystack).

Embedding and search methods will be designed to accommodate scaling to tens of thousands of notes without loss of performance. PCA-style reduction is not required because transformer-based embedding models (e.g., `text-embedding-3-small`) are already trained to compress meaning into ~1,000-dimensional space suitable for cosine similarity.

For now, all data is expected to be processed and stored locally. Cloud sync or long-term storage is considered out of scope for the MVP.